{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Pass on a four pixel (2x2) image<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tensors</h1> \n",
    "\n",
    "Tensors are just n dimensional arrays, just like in NumPy or matrices in linear algebra.\n",
    "We can perform all sorts of mathematical operations on tensors, like matrix multiplication, transposing, etc.\n",
    "\n",
    "\n",
    "Anything that can be represented as a tensor is a good candidate to be the input of a neural net. \n",
    "\n",
    "For our very first neural network that we're going to build from scratch we'll create classifier that is able to distinguing between 4 different types of images (our images will be <b>very</b> small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and a few submodules for convinience\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import numpy for converting torch tensors to arrays that can be displayed\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot to plot things\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a couple of tensors\n",
    "A = [[1, 2, 1],[0, 1, 0],[2, 3, 4]]\n",
    "tensor_A = torch.tensor(A)\n",
    "\n",
    "A = [[2, 5, 1],[6, 7, 1],[1, 8, 1]]\n",
    "tensor_B = torch.tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 10,  1],\n",
      "        [ 0,  7,  0],\n",
      "        [ 2, 24,  4]])\n"
     ]
    }
   ],
   "source": [
    "# Element Wise Multiplication\n",
    "print(tensor_A * tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 27,  4],\n",
      "        [ 6,  7,  1],\n",
      "        [26, 63,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication (Dot product)\n",
    "print(tensor_A @ tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(tensor_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Image classifier </h1>\n",
    "    \n",
    " We'll start with a 2 layer fully connected network that can classify between solid, vertical, diagonal and horizontal images. \n",
    " \n",
    " <img src=\"imgs/network.jpg\">\n",
    " \n",
    " <br> Bellow is a sample input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-405a383c2c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Make it into a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Create our input image\n",
    "image_array = [[-1, -1], \n",
    "               [1, 1]]\n",
    "\n",
    "image_array = [[.5, 0], \n",
    "               [.75, -.75]]\n",
    "\n",
    "\n",
    "# Make it into a tensor\n",
    "image_tensor = torch.Tensor(image_array)\n",
    "plt.imshow(image_array, cmap='gray', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example will be using a Fully Connected (FC) Layer. An FC layer is called Linear in PyTorch.\n",
    "To take advantage of a FC layer we must first display our image as a vector. Note that the simplest form of a FC layer is a single neuron.\n",
    "\n",
    "<img src=\"./imgs/input_vector.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000,  0.0000,  0.7500, -0.7500])\n"
     ]
    }
   ],
   "source": [
    "# Flatten the image\n",
    "x0 = torch.flatten(image_tensor)\n",
    "\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've flattened our image. Let's use it as input to a single neuron. Our example neuron takes a weighted sum of our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/neuron.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0750, grad_fn=<DotBackward>)\n",
      "-1.0750000000000002\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Layer\n",
    "# with 4 inputs and 1 output\n",
    "fc0 = nn.Linear(4, 1, bias = False)\n",
    "\n",
    "# Layers are usualy randomly initialized but for this example we will hard code the weights\n",
    "fc0.weight.data = torch.Tensor([-.2, 0, -.5, .8])\n",
    "\n",
    "# We can also represent a neuron or a fully connected layer as an array\n",
    "A = [-.2, 0, -.5, .8]\n",
    "\n",
    "# Applying the function fc0 to x yields the same result as the matrix multiplicaiton of x and A\n",
    "x1 = fc0(x0)\n",
    "print(x1)\n",
    "print(np.array(x0) @ A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "We often simplify our tensors by constraning them to a range.\n",
    "<img src=\"imgs/sigmoid.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7913, grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summerize what we've done so far:\n",
    "- Take an input image and flatten it into a vector\n",
    "- Define a neuron with hardcoded weights\n",
    "- Feed our input vector to the neuron\n",
    "- Pass the output of the neuron through a squashing function\n",
    "\n",
    "Believe it or not, we have all (most) of the components we need to create a full neural network. Now we'll scale up from a single neuron to 2 FC layers with 4 neurons each.\n",
    "\n",
    "Our first fully connected layer will consist of 4 neurons that activate when a certain combination of pixels is encountered in the input image.\n",
    "\n",
    "<img src=\"./imgs/fc1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2545, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through sigmoid activation function\n",
    "x1 = torch.sigmoid(x1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/fc2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2500, -0.7500, -0.2500,  0.7500], grad_fn=<SqueezeBackward3>)\n",
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1.]], requires_grad=True)\n",
      "tensor([ 1.2500, -0.7500, -0.2500,  0.7500], grad_fn=<SqueezeBackward3>)\n",
      "tensor([ 1., -1.,  3.,  0.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(x1)\n",
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 1, 1],\n",
    "                                [-1, 1, -1, 1],\n",
    "                                [1, -1, -1, 1],\n",
    "                                [1, 1, -1, -1],])\n",
    "\n",
    "print(fc2.weight)\n",
    "x1 = fc1(x0)\n",
    "x2 = fc2(x1)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(x2)\n",
    "torch.relu(x2)\n",
    "\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000, -0.9640, -0.9640], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[-1, -1], \n",
    "                   [1, 1]])\n",
    "x = torch.flatten(x)\n",
    "\n",
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n",
    "x = fc1(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., -1.],\n",
      "        [ 0.,  0.,  1.,  1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000, -0.9586], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                [-1, 1, 0, 0],\n",
    "                                [0, 0, 1, -1],\n",
    "                                [0, 0, 1, 1]])\n",
    "\n",
    "# Weights might be wrong\n",
    "print(fc2.weight)\n",
    "\n",
    "x = fc2(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9586],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Layer\n",
    "fc3 = nn.Linear(4, 8, bias = False)\n",
    "fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                [-1, 0, 0, 0],\n",
    "                                [0, 1, 0, 0],\n",
    "                                [0, -1, 0, 0],\n",
    "                                [0, 0, 1, 0],\n",
    "                                [0, 0, -1, 0],\n",
    "                                [0, 0, 0, 1],\n",
    "                                [0, 0, 0, -1]])\n",
    "\n",
    "x = fc3(x)\n",
    "x = F.relu(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    \n",
    "    # Flatten\n",
    "    x = torch.flatten(x)\n",
    "    \n",
    "    # Define fc1\n",
    "    fc1 = nn.Linear(4, 4, bias = False)\n",
    "    fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                    [0, 1, 0, 1],\n",
    "                                    [1, 0, -1, 0],\n",
    "                                    [0, 1, 0, -1],])\n",
    "    # Apply fc1\n",
    "    x = fc1(x)\n",
    "    # Apply activation function to output of fc1\n",
    "    x = torch.tanh(x)\n",
    "    \n",
    "    # Define fc2\n",
    "    fc2 = nn.Linear(4, 4, bias = False)\n",
    "    fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                    [-1, 1, 0, 0],\n",
    "                                    [0, 0, 1, -1],\n",
    "                                    [0, 0, 1, 1]])\n",
    "    # Apply fc2\n",
    "    x = fc2(x)\n",
    "    # Apply activation function to output of fc2\n",
    "    x = torch.tanh(x)\n",
    "    xfc2 = x\n",
    "    \n",
    "    # Output layer\n",
    "    fc3 = nn.Linear(4, 8, bias = False)\n",
    "    fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                    [-1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, -1, 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, -1, 0],\n",
    "                                    [0, 0, 0, 1],\n",
    "                                    [0, 0, 0, -1]])\n",
    "\n",
    "    x = fc3(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    return x, xfc2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(x):\n",
    "    \n",
    "    cls = list(x).index(max(x))\n",
    "    \n",
    "    if cls == 0 or cls == 1:\n",
    "        return \"solid\"\n",
    "    if cls == 2 or cls == 3:\n",
    "        return \"vertical\"\n",
    "    if cls == 4 or cls == 5:\n",
    "        return \"diagonal\"\n",
    "    if cls == 6 or cls ==7:\n",
    "        return \"horizontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9586],\n",
    "\n",
    "output_L = nn.Linear(4, 8, bias = False)\n",
    "output_L.weight.data = torch.Tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0, 0, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000, -0.9586], grad_fn=<TanhBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 8x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0b1b403206a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_L\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 8x8)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a0d74accc198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlabel_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "x = [[-1, 1], \n",
    "     [1, -1]]\n",
    "x = torch.Tensor(x)\n",
    "x = forward(x)\n",
    "label_map(np.array(x.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
