{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Pass on a four pixel (2x2) image<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tensors</h1> \n",
    "\n",
    "Tensors are just n dimensional arrays, just like in NumPy or matrices in linear algebra.\n",
    "We can perform all sorts of mathematical operations on tensors, like matrix multiplication, transposing, etc.\n",
    "\n",
    "\n",
    "Anything that can be represented as a tensor is a good candidate to be the input of a neural net. \n",
    "\n",
    "For our very first neural network that we're going to build from scratch we'll create classifier that is able to distinguing between 4 different types of images (our images will be <b>very</b> small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and a few submodules for convinience\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import numpy for converting torch tensors to arrays that can be displayed\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot to plot things\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a couple of tensors\n",
    "A = [[1, 2, 1],[0, 1, 0],[2, 3, 4]]\n",
    "tensor_A = torch.tensor(A)\n",
    "\n",
    "A = [[2, 5, 1],[6, 7, 1],[1, 8, 1]]\n",
    "tensor_B = torch.tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 10,  1],\n",
      "        [ 0,  7,  0],\n",
      "        [ 2, 24,  4]])\n"
     ]
    }
   ],
   "source": [
    "# Element Wise Multiplication\n",
    "print(tensor_A * tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 27,  4],\n",
      "        [ 6,  7,  1],\n",
      "        [26, 63,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication (Dot product)\n",
    "print(tensor_A @ tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(tensor_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Image classifier </h1>\n",
    "    \n",
    " We'll start with a 2 layer fully connected network that can classify between solid, vertical, diagonal and horizontal images. \n",
    " \n",
    " <img src=\"imgs/network.jpg\">\n",
    " \n",
    " <br> Bellow is a sample input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create our input image\n",
    "image_array = [[-1, -1], \n",
    "               [1, 1]]\n",
    "\n",
    "image_array = [[.5, 0], \n",
    "               [.75, -.75]]\n",
    "\n",
    "\n",
    "# Make it into a tensor\n",
    "image_tensor = torch.Tensor(image_array)\n",
    "plt.imshow(image_array, cmap='gray', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example will be using a Fully Connected (FC) Layer. An FC layer is called Linear in PyTorch.\n",
    "To take advantage of a FC layer we must first display our image as a vector. Note that the simplest form of a FC layer is a single neuron.\n",
    "\n",
    "<img src=\"./imgs/input_vector.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000,  0.0000,  0.7500, -0.7500])\n"
     ]
    }
   ],
   "source": [
    "# Flatten the image\n",
    "x0 = torch.flatten(image_tensor)\n",
    "\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've flattened our image. Let's use it as input to a single neuron. Our example neuron takes a weighted sum of our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/neuron.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0750, grad_fn=<DotBackward>)\n",
      "-1.0750000000000002\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Layer\n",
    "# with 4 inputs and 1 output\n",
    "fc0 = nn.Linear(4, 1, bias = False)\n",
    "\n",
    "# Layers are usualy randomly initialized but for this example we will hard code the weights\n",
    "fc0.weight.data = torch.Tensor([-.2, 0, -.5, .8])\n",
    "\n",
    "# We can also represent a neuron or a fully connected layer as an array\n",
    "A = [-.2, 0, -.5, .8]\n",
    "\n",
    "# Applying the function fc0 to x yields the same result as the matrix multiplicaiton of x and A\n",
    "x1 = fc0(x0)\n",
    "print(x1)\n",
    "print(np.array(x0) @ A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "We often simplify our tensors by constraning them to a range.\n",
    "<img src=\"imgs/sigmoid.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7913, grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summerize what we've done so far:\n",
    "- Take an input image and flatten it into a vector\n",
    "- Define a neuron with hardcoded weights\n",
    "- Feed our input vector to the neuron\n",
    "- Pass the output of the neuron through a squashing function\n",
    "\n",
    "Believe it or not, we have all (most) of the components we need to create a full neural network. Now we'll scale up from a single neuron to 2 FC layers with 4 neurons each.\n",
    "\n",
    "Our first fully connected layer will consist of 4 neurons that activate when a certain combination of pixels is encountered in the input image.\n",
    "\n",
    "<img src=\"./imgs/fc1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2545, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through sigmoid activation function\n",
    "x1 = torch.sigmoid(x1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/fc2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2545, grad_fn=<SigmoidBackward>)\n",
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1.]], requires_grad=True)\n",
      "tensor([ 1.2500, -0.7500, -0.2500,  0.7500], grad_fn=<SqueezeBackward3>)\n",
      "tensor([ 1., -1.,  3.,  0.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(x1)\n",
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 1, 1],\n",
    "                                [-1, 1, -1, 1],\n",
    "                                [1, -1, -1, 1],\n",
    "                                [1, 1, -1, -1],])\n",
    "\n",
    "print(fc2.weight)\n",
    "x1 = fc1(x0)\n",
    "x2 = fc2(x1)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -1.,  3.,  0.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(x2)\n",
    "torch.relu(x2)\n",
    "\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000, -0.9640, -0.9640], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[-1, -1], \n",
    "                   [1, 1]])\n",
    "x = torch.flatten(x)\n",
    "\n",
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n",
    "x = fc1(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., -1.],\n",
      "        [ 0.,  0.,  1.,  1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000, -0.9586], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                [-1, 1, 0, 0],\n",
    "                                [0, 0, 1, -1],\n",
    "                                [0, 0, 1, 1]])\n",
    "\n",
    "# Weights might be wrong\n",
    "print(fc2.weight)\n",
    "\n",
    "x = fc2(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9586],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Layer\n",
    "fc3 = nn.Linear(4, 8, bias = False)\n",
    "fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                [-1, 0, 0, 0],\n",
    "                                [0, 1, 0, 0],\n",
    "                                [0, -1, 0, 0],\n",
    "                                [0, 0, 1, 0],\n",
    "                                [0, 0, -1, 0],\n",
    "                                [0, 0, 0, 1],\n",
    "                                [0, 0, 0, -1]])\n",
    "\n",
    "x = fc3(x)\n",
    "x = F.relu(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    \n",
    "    # Flatten\n",
    "    x = torch.flatten(x)\n",
    "    \n",
    "    # Define fc1\n",
    "    fc1 = nn.Linear(4, 4, bias = False)\n",
    "    fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                    [0, 1, 0, 1],\n",
    "                                    [1, 0, -1, 0],\n",
    "                                    [0, 1, 0, -1],])\n",
    "    # Apply fc1\n",
    "    x = fc1(x)\n",
    "    # Apply activation function to output of fc1\n",
    "    x = torch.tanh(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Define fc2\n",
    "    fc2 = nn.Linear(4, 4, bias = False)\n",
    "    fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                    [-1, 1, 0, 0],\n",
    "                                    [0, 0, 1, -1],\n",
    "                                    [0, 0, 1, 1]])\n",
    "    # Apply fc2\n",
    "    x = fc2(x)\n",
    "    # Apply activation function to output of fc2\n",
    "    x = torch.tanh(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Output layer\n",
    "    fc3 = nn.Linear(4, 8, bias = False)\n",
    "    fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                    [-1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, -1, 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, -1, 0],\n",
    "                                    [0, 0, 0, 1],\n",
    "                                    [0, 0, 0, -1]])\n",
    "\n",
    "    x = fc3(x)\n",
    "    x = F.relu(x)\n",
    "    print(fc3.weight.data.shape)\n",
    "    print(fc3.weight.data)\n",
    "    print(x.shape)\n",
    "    \n",
    "    fc4 = nn.Linear(8, 4, bias=False)\n",
    "    fc4.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                    [1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 0, 1],\n",
    "                                    [0, 0, 0, 1]])\n",
    "    print(fc4.weight.data.shape)\n",
    "    print(fc4.weight.data)\n",
    "    print(x.shape)\n",
    "    x = fc4(x)\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(x):\n",
    "    \n",
    "    cls = list(x).index(max(x))\n",
    "    \n",
    "    if cls == 0 or cls == 1:\n",
    "        return \"solid\"\n",
    "    if cls == 2 or cls == 3:\n",
    "        return \"vertical\"\n",
    "    if cls == 4 or cls == 5:\n",
    "        return \"diagonal\"\n",
    "    if cls == 6 or cls ==7:\n",
    "        return \"horizontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([8, 4])\n",
      "tensor([[ 1.,  0.,  0.,  0.],\n",
      "        [-1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., -1.]])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 4])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "torch.Size([8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x8 and 4x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a0d74accc198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m      [1, -1]]\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-002eef8f524e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x8 and 4x8)"
     ]
    }
   ],
   "source": [
    "x = [[-1, 1], \n",
    "     [1, -1]]\n",
    "x = torch.Tensor(x)\n",
    "x = forward(x)\n",
    "label_map(np.array(x.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
