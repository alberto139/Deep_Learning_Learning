{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Pass on a four pixel (2x2) image<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tensors</h1> \n",
    "\n",
    "Tensors are just n dimensional arrays, just like in NumPy or matrices in linear algebra.\n",
    "We can perform all sorts of mathematical operations on tensors, like matrix multiplication, transposing, etc.\n",
    "\n",
    "\n",
    "Anything that can be represented as a tensor is a good candidate to be the input of a neural net. \n",
    "\n",
    "For our very first neural network that we're going to build from scratch we'll create classifier that is able to distinguing between 4 different types of images (our images will be <b>very</b> small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and a few submodules for convinience\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import numpy for converting torch tensors to arrays that can be displayed\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot to plot things\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a couple of tensors\n",
    "A = [[1, 2, 1],[0, 1, 0],[2, 3, 4]]\n",
    "tensor_A = torch.tensor(A)\n",
    "\n",
    "A = [[2, 5, 1],[6, 7, 1],[1, 8, 1]]\n",
    "tensor_B = torch.tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 10,  1],\n",
      "        [ 0,  7,  0],\n",
      "        [ 2, 24,  4]])\n"
     ]
    }
   ],
   "source": [
    "# Element Wise Multiplication\n",
    "print(tensor_A * tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 27,  4],\n",
      "        [ 6,  7,  1],\n",
      "        [26, 63,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication (Dot product)\n",
    "print(tensor_A @ tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(tensor_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Image classifier </h1>\n",
    "    \n",
    " We'll start with a 2 layer fully connected network that can classify between solid, vertical, diagonal and horizontal images. \n",
    " \n",
    " <img src=\"imgs/network.jpg\">\n",
    " \n",
    " <br> Bellow is a sample input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create our input image\n",
    "image_array = [[-1, -1], \n",
    "               [1, 1]]\n",
    "\n",
    "image_array = [[.5, 0], \n",
    "               [.75, -.75]]\n",
    "\n",
    "\n",
    "# Make it into a tensor\n",
    "image_tensor = torch.Tensor(image_array)\n",
    "plt.imshow(image_array, cmap='gray', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example will be using a Fully Connected (FC) Layer. An FC layer is called Linear in PyTorch.\n",
    "To take advantage of a FC layer we must first display our image as a vector. Note that the simplest form of a FC layer is a single neuron.\n",
    "\n",
    "<img src=\"./imgs/input_vector.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000,  0.0000,  0.7500, -0.7500])\n"
     ]
    }
   ],
   "source": [
    "# Flatten the image\n",
    "x0 = torch.flatten(image_tensor)\n",
    "\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've flattened our image. Let's use it as input to a single neuron. Our example neuron takes a weighted sum of our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/neuron.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0750, grad_fn=<DotBackward>)\n",
      "-1.0750000000000002\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Layer\n",
    "# with 4 inputs and 1 output\n",
    "fc0 = nn.Linear(4, 1, bias = False)\n",
    "\n",
    "# Layers are usualy randomly initialized but for this example we will hard code the weights\n",
    "fc0.weight.data = torch.Tensor([-.2, 0, -.5, .8])\n",
    "\n",
    "# We can also represent a neuron or a fully connected layer as an array\n",
    "A = [-.2, 0, -.5, .8]\n",
    "\n",
    "# Applying the function fc0 to x yields the same result as the matrix multiplicaiton of x and A\n",
    "x1 = fc0(x0)\n",
    "print(x1)\n",
    "print(np.array(x0) @ A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "We often simplify our tensors by constraning them to a range.\n",
    "<img src=\"imgs/sigmoid.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7913, grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summerize what we've done so far:\n",
    "- Take an input image and flatten it into a vector\n",
    "- Define a neuron with hardcoded weights\n",
    "- Feed our input vector to the neuron\n",
    "- Pass the output of the neuron through a squashing function\n",
    "\n",
    "Believe it or not, we have all (most) of the components we need to create a full neural network. Now we'll scale up from a single neuron to 2 FC layers with 4 neurons each.\n",
    "\n",
    "Our first fully connected layer will consist of 4 neurons that activate when a certain combination of pixels is encountered in the input image.\n",
    "\n",
    "<img src=\"./imgs/fc1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2545, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through sigmoid activation function\n",
    "x1 = torch.sigmoid(x1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/fc2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2545, grad_fn=<SigmoidBackward>)\n",
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1.]], requires_grad=True)\n",
      "tensor([ 1.2500, -0.7500, -0.2500,  0.7500], grad_fn=<SqueezeBackward3>)\n",
      "tensor([ 1., -1.,  3.,  0.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(x1)\n",
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 1, 1],\n",
    "                                [-1, 1, -1, 1],\n",
    "                                [1, -1, -1, 1],\n",
    "                                [1, 1, -1, -1],])\n",
    "\n",
    "print(fc2.weight)\n",
    "x1 = fc1(x0)\n",
    "x2 = fc2(x1)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -1.,  3.,  0.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(x2)\n",
    "torch.relu(x2)\n",
    "\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0., -1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000, -0.9640, -0.9640], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[-1, -1], \n",
    "                   [1, 1]])\n",
    "x = torch.flatten(x)\n",
    "\n",
    "fc1 = nn.Linear(4, 4, bias = False)\n",
    "fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [1, 0, -1, 0],\n",
    "                                [0, 1, 0, -1],])\n",
    "print(fc1.weight)\n",
    "x = fc1(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., -1.],\n",
      "        [ 0.,  0.,  1.,  1.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000, -0.9586], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2 = nn.Linear(4, 4, bias = False)\n",
    "fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                [-1, 1, 0, 0],\n",
    "                                [0, 0, 1, -1],\n",
    "                                [0, 0, 1, 1]])\n",
    "\n",
    "# Weights might be wrong\n",
    "print(fc2.weight)\n",
    "\n",
    "x = fc2(x)\n",
    "x = torch.tanh(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9586],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Layer\n",
    "fc3 = nn.Linear(4, 8, bias = False)\n",
    "fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                [-1, 0, 0, 0],\n",
    "                                [0, 1, 0, 0],\n",
    "                                [0, -1, 0, 0],\n",
    "                                [0, 0, 1, 0],\n",
    "                                [0, 0, -1, 0],\n",
    "                                [0, 0, 0, 1],\n",
    "                                [0, 0, 0, -1]])\n",
    "\n",
    "x = fc3(x)\n",
    "x = F.relu(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    \n",
    "    # Flatten\n",
    "    x = torch.flatten(x)\n",
    "    \n",
    "    # Define fc1\n",
    "    fc1 = nn.Linear(4, 4, bias = False)\n",
    "    fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                    [0, 1, 0, 1],\n",
    "                                    [1, 0, -1, 0],\n",
    "                                    [0, 1, 0, -1],])\n",
    "    # Apply fc1\n",
    "    x = fc1(x)\n",
    "    # Apply activation function to output of fc1\n",
    "    x = torch.tanh(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Define fc2\n",
    "    fc2 = nn.Linear(4, 4, bias = False)\n",
    "    fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                    [-1, 1, 0, 0],\n",
    "                                    [0, 0, 1, -1],\n",
    "                                    [0, 0, 1, 1]])\n",
    "    # Apply fc2\n",
    "    x = fc2(x)\n",
    "    # Apply activation function to output of fc2\n",
    "    x = torch.tanh(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Output layer\n",
    "    fc3 = nn.Linear(4, 8, bias = False)\n",
    "    fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                    [-1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, -1, 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, -1, 0],\n",
    "                                    [0, 0, 0, 1],\n",
    "                                    [0, 0, 0, -1]])\n",
    "\n",
    "    x = fc3(x)\n",
    "    x = F.relu(x)\n",
    "    print(fc3.weight.data.shape)\n",
    "    print(fc3.weight.data)\n",
    "    print(x.shape)\n",
    "    \n",
    "    fc4 = nn.Linear(8, 4, bias=False)\n",
    "    fc4.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                    [1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, 1, 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 0, 1],\n",
    "                                    [0, 0, 0, 1]])\n",
    "    print(fc4.weight.data.shape)\n",
    "    print(fc4.weight.data)\n",
    "    print(x.shape)\n",
    "    x = fc4(x)\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(x):\n",
    "    \n",
    "    cls = list(x).index(max(x))\n",
    "    \n",
    "    if cls == 0 :\n",
    "        return \"solid\"\n",
    "    if cls == 1:\n",
    "        return \"vertical\"\n",
    "    if cls == 2 :\n",
    "        return \"diagonal\"\n",
    "    if cls == 3:\n",
    "        return \"horizontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 4, bias = False)\n",
    "        self.fc1.weight.data = torch.Tensor([[1, 0, 1, 0],\n",
    "                                            [0, 1, 0, 1],\n",
    "                                            [1, 0, -1, 0],\n",
    "                                            [0, 1, 0, -1],])\n",
    "        \n",
    "        self.fc2 = nn.Linear(4, 4, bias = False)\n",
    "        self.fc2.weight.data = torch.Tensor([[1, 1, 0, 0],\n",
    "                                            [-1, 1, 0, 0],\n",
    "                                            [0, 0, 1, -1],\n",
    "                                            [0, 0, 1, 1]])\n",
    "\n",
    "        self.fc3 = nn.Linear(4, 8, bias = False)\n",
    "        self.fc3.weight.data = torch.Tensor([[1, 0, 0, 0],\n",
    "                                            [-1, 0, 0, 0],\n",
    "                                            [0, 1, 0, 0],\n",
    "                                            [0, -1, 0, 0],\n",
    "                                            [0, 0, 1, 0],\n",
    "                                            [0, 0, -1, 0],\n",
    "                                            [0, 0, 0, 1],\n",
    "                                            [0, 0, 0, -1]])\n",
    "\n",
    "        self.fc4 = nn.Linear(8, 4, bias = False)\n",
    "        #print(self.fc4.weight.data)\n",
    "        #\"\"\"\"\n",
    "        self.fc4.weight.data = torch.Tensor([[1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                            [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "                                            [0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                                            [0, 0, 0, 0, 0, 0, 1, 1]])\n",
    "        #\"\"\"\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x = torch.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x) # First fully connected layer\n",
    "        x = torch.tanh(x) # Activation\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        #x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1997, 0.9385, 0.6420, 0.6420], grad_fn=<SqueezeBackward3>)\n",
      "vertical\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = [[1, 0], \n",
    "     [1, -1]]\n",
    "x = net.forward(torch.Tensor(image))\n",
    "print(x)\n",
    "cls = label_map(np.array(x.detach()))\n",
    "print(cls)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
